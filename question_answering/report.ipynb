{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hyk2vRT_8rGb"
   },
   "source": [
    "# Отчёт по домашнему заданию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cCUqt-0-kkZ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vB534oz9rm6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ba9K8Kp59xhX"
   },
   "outputs": [],
   "source": [
    "!cp /content/gdrive/My\\ Drive/abbyy-nlp/train.jsonl /content\n",
    "!cp /content/gdrive/My\\ Drive/abbyy-nlp/dev.jsonl /content\n",
    "!cp /content/gdrive/My\\ Drive/abbyy-nlp/wiki.en.zip /content\n",
    "!unzip wiki.en.zip\n",
    "!rm wiki.en.zip\n",
    "!rm wiki.en.vec\n",
    "\n",
    "!pip install gensim\n",
    "!pip install transformers\n",
    "!pip install nltk\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bS95jZvZ-vMz"
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AiHwkxFt8qgj"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "with open(\"train.jsonl\", ) as reader:\n",
    "    train_dataset = list(map(json.loads, reader))\n",
    "\n",
    "with open(\"dev.jsonl\", ) as reader:\n",
    "    dev_dataset = list(map(json.loads, reader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 903
    },
    "colab_type": "code",
    "id": "BHJRa9k-xken",
    "outputId": "146df40c-5fd0-42b9-e49c-05d650406b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры из обучающей выборки: \n",
      "\n",
      "{'answer': True,\n",
      " 'passage': 'This is a list of all penalty shoot-outs that have occurred in '\n",
      "            'the Finals tournament of the FIFA World Cup. Penalty shoot-outs '\n",
      "            'were introduced as tie-breakers in the 1978 World Cup but did not '\n",
      "            'occur before 1982. The first time a World Cup title was won by '\n",
      "            'penalty shoot-out was in 1994. The only other time was in 2006. '\n",
      "            'By the end of the 2018 edition, 30 shoot-outs have taken place in '\n",
      "            'the World Cup. Of these, only two reached the sudden death stage '\n",
      "            \"after still being tied at the end of ``best of five kicks''.\",\n",
      " 'question': 'does the world cup final go to penalties',\n",
      " 'title': 'List of FIFA World Cup penalty shoot-outs'}\n",
      "{'answer': True,\n",
      " 'passage': 'Clobetasol propionate /kloʊˈbeɪtəsɒl/ is a corticosteroid of the '\n",
      "            'glucocorticoid class used to treat various skin disorders '\n",
      "            'including eczema and psoriasis. It is also highly effective for '\n",
      "            'contact dermatitis caused by exposure to poison ivy/oak. '\n",
      "            'Clobetasol belongs to US Class I (Europe: class IV) of the '\n",
      "            'corticosteroids, making it one of the most potent available. It '\n",
      "            'comes in shampoo, mousse, ointment and emollient cream '\n",
      "            'presentations. It has very high potency and typically should not '\n",
      "            'be used with occlusive dressings, or for extended continuous use '\n",
      "            '(beyond two weeks). It is also used to treat several autoimmune '\n",
      "            'diseases including alopecia areata, lichen sclerosus, and lichen '\n",
      "            'planus.',\n",
      " 'question': 'is clobetasol propionate cream good for poison ivy',\n",
      " 'title': 'Clobetasol propionate'}\n",
      "\n",
      " ================================================== \n",
      "\n",
      "Примеры из тестовой выборки: \n",
      "\n",
      "{'answer': False,\n",
      " 'passage': 'In mathematics, and more specifically set theory, the empty set '\n",
      "            'or null set is the unique set having no elements; its size or '\n",
      "            'cardinality (count of elements in a set) is zero. Some axiomatic '\n",
      "            'set theories ensure that the empty set exists by including an '\n",
      "            'axiom of empty set; in other theories, its existence can be '\n",
      "            'deduced. Many possible properties of sets are vacuously true for '\n",
      "            'the empty set.',\n",
      " 'question': 'is the empty set an element of the set containing the empty set',\n",
      " 'title': 'Empty set'}\n",
      "{'answer': True,\n",
      " 'passage': 'The Kissing Booth is a 2018 American romantic comedy film '\n",
      "            'directed by Vince Marcello, based on the novel of the same name '\n",
      "            'by Beth Reekles. It stars Molly Ringwald, Joey King, Jacob Elordi '\n",
      "            'and Joel Courtney. The film was released on May 11, 2018 on '\n",
      "            'Netflix.',\n",
      " 'question': 'is the movie the kissing booth on netflix',\n",
      " 'title': 'The Kissing Booth'}\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "n_samples = 2\n",
    "print(\"Примеры из обучающей выборки: \\n\")\n",
    "_ = [pprint(train_dataset[randint(0, len(train_dataset)-1)]) for _ in range(n_samples)]\n",
    "print(\"\\n\", \"=\"*50, \"\\n\")\n",
    "print(\"Примеры из тестовой выборки: \\n\")\n",
    "_ = [pprint(dev_dataset[randint(0, len(dev_dataset)-1)]) for _ in range(n_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_G9Wfo3Q-7Po"
   },
   "source": [
    "# Часть 1. [1 балл] Эксплоративный анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4DpEdWY6_gRZ"
   },
   "source": [
    "## 1.1 Доля yes и no классов в корпусе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "mOmrzmJT-wz5",
    "outputId": "5a48fb5e-be67-4366-e5fd-c31c8b446cc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка\n",
      "\t yes: 0.6231038506417736\n",
      "\t no: 0.37689614935822635\n",
      "Тестовая выборка\n",
      "\t yes: 0.6217125382262997\n",
      "\t no: 0.3782874617737003\n"
     ]
    }
   ],
   "source": [
    "get_yes_ratio = lambda ds: sum(map(lambda x: x['answer'], ds)) / len(ds)\n",
    "\n",
    "print(\"Обучающая выборка\")\n",
    "print(\"\\t yes:\", get_yes_ratio(train_dataset))\n",
    "print(f\"\\t no:\", 1 - get_yes_ratio(train_dataset))\n",
    "\n",
    "print(\"Тестовая выборка\")\n",
    "print(f\"\\t yes:\", get_yes_ratio(dev_dataset))\n",
    "print(f\"\\t no:\", 1 - get_yes_ratio(dev_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWCu48piBJBN"
   },
   "source": [
    "## 1.2-3 Средняя длина вопроса и параграфа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "HUFAgLDiBEPW",
    "outputId": "070ce4cc-debe-4f97-b4d1-d41e2aec210a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя длина вопроса:\n",
      "\t в обучающей выборке:\n",
      "\t\t word: 8.831971995332555\n",
      "\t\t char: 43.99193805028111\n",
      "\t в тестовой выборке:\n",
      "\t\t word: 8.726299694189603\n",
      "\t\t char: 43.206422018348626\n",
      "Средняя длина параграфа:\n",
      "\t в обучающей выборке:\n",
      "\t\t word: 109.20685265726105\n",
      "\t\t char: 565.6130264134931\n",
      "\t в тестовой выборке:\n",
      "\t\t word: 108.27033639143731\n",
      "\t\t char: 559.0522935779817\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "get_word_count = lambda paragraph, field: len(tokenizer.tokenize(paragraph[field]))\n",
    "get_avg_word_count = lambda ds, field: sum(map(lambda x: get_word_count(x, field), ds)) / len(ds)\n",
    "get_avg_char_count = lambda ds, field: sum(map(lambda x: len(x[field]), ds)) / len(ds)\n",
    "\n",
    "print(\"Средняя длина вопроса:\")\n",
    "print(\"\\t в обучающей выборке:\")\n",
    "print(\"\\t\\t word:\", get_avg_word_count(train_dataset, 'question'))\n",
    "print(\"\\t\\t char:\", get_avg_char_count(train_dataset, 'question'))\n",
    "\n",
    "print(\"\\t в тестовой выборке:\")\n",
    "print(\"\\t\\t word:\", get_avg_word_count(dev_dataset, 'question'))\n",
    "print(\"\\t\\t char:\", get_avg_char_count(dev_dataset, 'question'))\n",
    "\n",
    "print(\"Средняя длина параграфа:\")\n",
    "print(\"\\t в обучающей выборке:\")\n",
    "print(\"\\t\\t word:\", get_avg_word_count(train_dataset, 'passage'))\n",
    "print(\"\\t\\t char:\", get_avg_char_count(train_dataset, 'passage'))\n",
    "\n",
    "print(\"\\t в тестовой выборке:\")\n",
    "print(\"\\t\\t word:\", get_avg_word_count(dev_dataset, 'passage'))\n",
    "print(\"\\t\\t char:\", get_avg_char_count(dev_dataset, 'passage'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyFERhPswTVV"
   },
   "source": [
    "## Эвристики, по которым был собран датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RL8LSH6QzDdJ"
   },
   "source": [
    "Авторы статьи оставили только те вопросы, которые начинались с одного из слов {“did”, “do”, “does”, “is”, “are”, “was”, “were”, “have”, “has”, “can”, “could”, “will”, “would”}. После этого фильтровались вопросы, которые при запросе в поиске гугл, не выдавали страницу с википедией среди первых 5 выдач."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AhY8_YmUvSZv"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "starter_tokens = {\"did\", \"do\", \"does\", \"is\", \"are\", \"was\", \"were\", \"have\", \"has\", \"can\", \"could\", \"will\", \"would\"}\n",
    "first_question_words = Counter([doc['question'].split()[0] for doc in train_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSmPh26K45dp"
   },
   "source": [
    "### 20 наиболее часто встречающихся первых токенов вопроса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "Tbjo8PKM4G7e",
    "outputId": "9d924f32-5841-4558-e97a-22edd787d689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 4190),\n",
      " ('can', 1136),\n",
      " ('does', 952),\n",
      " ('are', 693),\n",
      " ('do', 664),\n",
      " ('did', 461),\n",
      " ('was', 335),\n",
      " ('has', 302),\n",
      " ('will', 181),\n",
      " ('the', 91),\n",
      " ('have', 70),\n",
      " ('in', 35),\n",
      " ('were', 25),\n",
      " ('if', 17),\n",
      " ('a', 16),\n",
      " ('what', 11),\n",
      " ('when', 10),\n",
      " ('could', 9),\n",
      " ('who', 5),\n",
      " ('where', 5)]\n"
     ]
    }
   ],
   "source": [
    "pprint(first_question_words.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bn4YNUiz4_0i"
   },
   "source": [
    "### Процент вопросов, начинающихся со слов \"did\", \"do\", \"does\", \"is\", \"are\", \"was\", \"were\", \"have\", \"has\", \"can\", \"could\", \"will\", \"would\"\n",
    "\n",
    "Однако, несмотря на заявления авторов даатсета и статьи, в датасете также присутствуют вопросы, начинающиеся не с других слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VqUVSvRl4Ipo",
    "outputId": "fc9b69ec-2ca3-428b-93cf-7583bf8a2aa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9570382942611647"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([first_question_words[word] for word in starter_tokens]) / len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ombGYE_Q5PvQ"
   },
   "source": [
    "### Примеры вопросов, которые начинаются с других слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bwR7sFhC1frF",
    "outputId": "5d9e4ee0-c2c1-4cc5-a0e5-58c58d4fb6f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': False,\n",
      " 'passage': 'Quantico has received positive reviews from critics, with praise '\n",
      "            \"for Chopra's performance and the diversity of the cast. However, \"\n",
      "            \"the ``confusing dual timelines'' received some criticism. The \"\n",
      "            \"series has been nominated for four People's Choice Awards, with \"\n",
      "            'Chopra winning two: Favorite Actress in a New TV Series in '\n",
      "            \"2016--making her the first South Asian to win a People's Choice \"\n",
      "            'Award--and Favorite Dramatic TV Actress in 2017. ABC renewed the '\n",
      "            'series for a third season, which premiered on April 26, 2018. As '\n",
      "            'part of the renewal process, Safran was replaced as primary '\n",
      "            'showrunner by Michael Seitzman. On May 11, 2018, ABC cancelled '\n",
      "            'the show after three seasons. The network will air the remaining '\n",
      "            'episodes of the third season on Friday nights.',\n",
      " 'question': 'what season of quantico is on tv now',\n",
      " 'title': 'Quantico (TV series)'}\n",
      "{'answer': True,\n",
      " 'passage': 'The British Agricultural Revolution, or Second Agricultural '\n",
      "            'Revolution, was the unprecedented increase in agricultural '\n",
      "            'production in Britain due to increases in labour and land '\n",
      "            'productivity between the mid-17th and late 19th centuries. '\n",
      "            'Agricultural output grew faster than the population over the '\n",
      "            'century to 1770, and thereafter productivity remained among the '\n",
      "            'highest in the world. This increase in the food supply '\n",
      "            'contributed to the rapid growth of population in England and '\n",
      "            'Wales, from 5.5 million in 1700 to over 9 million by 1801, though '\n",
      "            'domestic production gave way increasingly to food imports in the '\n",
      "            'nineteenth century as population more than tripled to over 32 '\n",
      "            'million. The rise in productivity accelerated the decline of the '\n",
      "            'agricultural share of the labour force, adding to the urban '\n",
      "            'workforce on which industrialization depended: the Agricultural '\n",
      "            'Revolution has therefore been cited as a cause of the Industrial '\n",
      "            'Revolution.',\n",
      " 'question': 'what relationship was there between improvements in agriculture '\n",
      "             'and the industrialisation of britain',\n",
      " 'title': 'British Agricultural Revolution'}\n",
      "{'answer': True,\n",
      " 'passage': 'The Supreme Court is the only federal court that is explicitly '\n",
      "            'mandated by the Constitution. During the Constitutional '\n",
      "            'Convention, a proposal was made for the Supreme Court to be the '\n",
      "            'only federal court, having both original jurisdiction and '\n",
      "            'appellate jurisdiction. This proposal was rejected in favor of '\n",
      "            'the provision that exists today. Under this provision, the '\n",
      "            'Congress may create inferior (i.e., lower) courts under both '\n",
      "            'Article III, Section 1, and Article I, Section 8. The Article III '\n",
      "            \"courts, which are also known as ``constitutional courts'', were \"\n",
      "            'first created by the Judiciary Act of 1789. Article I courts, '\n",
      "            \"which are also known as ``legislative courts'', consist of \"\n",
      "            'regulatory agencies, such as the United States Tax Court. Article '\n",
      "            'III courts are the only ones with judicial power, and so '\n",
      "            'decisions of regulatory agencies remain subject to review by '\n",
      "            'Article III courts. However, cases not requiring ``judicial '\n",
      "            \"determination'' may come before Article I courts. In the case of \"\n",
      "            \"Murray's Lessee v. Hoboken Land & Improvement Co. 59 U.S. 272 \"\n",
      "            '(1855), the Supreme Court ruled that cases involving ``a suit at '\n",
      "            \"the common law, or in equity, or admiralty'' inherently involve \"\n",
      "            'judicial determination and must come before Article III courts. '\n",
      "            'Other cases, such as bankruptcy cases, have been held not to '\n",
      "            'involve judicial determination, and may therefore go before '\n",
      "            'Article I courts. Similarly, several courts in the District of '\n",
      "            'Columbia, which is under the exclusive jurisdiction of the '\n",
      "            'Congress, are Article I courts rather than Article III courts. '\n",
      "            'This article was expressly extended to the United States District '\n",
      "            'Court for the District of Puerto Rico by the U.S. Congress '\n",
      "            'through Federal Law 89-571, 80 Stat. 764, signed by President '\n",
      "            'Lyndon B. Johnson in 1966. This transformed the article IV United '\n",
      "            'States territorial court in Puerto Rico, created in 1900, to an '\n",
      "            'Article III federal judicial district court.',\n",
      " 'question': 'the us supreme court is the only court established by '\n",
      "             'constitutional mandate',\n",
      " 'title': 'Article Three of the United States Constitution'}\n"
     ]
    }
   ],
   "source": [
    "from random import choices\n",
    "indices = [i for i, x in enumerate([doc['question'].split()[0] not in starter_tokens for doc in train_dataset]) if x]\n",
    "subsample = choices(indices, k=3)\n",
    "_ = [pprint(train_dataset[i]) for i in subsample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eu93dNW87ru3"
   },
   "source": [
    "# 2. Baseline\n",
    "Для того, чтобы сравнивать качество определения ответов, необходимо сначала получить первое приближение. В качестве таких приближений использованы два классификатора: Dummy Classifier и FastText+LogReg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pruX9Oe91ce"
   },
   "source": [
    "## 2.1 Dummy Classifier\n",
    "Dummy classifier представляет собой классификатор, который каждой новой паре (вопрос, контекст) предсказывает наиболее частый класс из обучающей выборки. При этом, если в качество алгоритма на тестовой выборке будет равно процентному соотношению самого частого класса в тестовой выборке, т.е. 0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytbCNKcI8sKb"
   },
   "outputs": [],
   "source": [
    "train_questions = [doc['question'] for doc in train_dataset]\n",
    "train_passages = [doc['passage'] for doc in train_dataset]\n",
    "train_answers = [str(doc['answer']) for doc in train_dataset]\n",
    "\n",
    "dev_questions = [doc['question'] for doc in dev_dataset]\n",
    "dev_passages = [doc['passage'] for doc in dev_dataset]\n",
    "dev_answers = [str(doc['answer']) for doc in dev_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m9pcPUR84a3x",
    "outputId": "cdcdb82e-afe3-4ba8-c8e4-b98ddb91fd80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(constant=None, random_state=None, strategy='most_frequent')"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "clf.fit(train_questions, train_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "cAxsKYwx98lj",
    "outputId": "92a96d12-ae55-4e92-a8b3-a21330581482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False     0.0000    0.0000    0.0000      1237\n",
      "        True     0.6217    1.0000    0.7667      2033\n",
      "\n",
      "    accuracy                         0.6217      3270\n",
      "   macro avg     0.3109    0.5000    0.3834      3270\n",
      "weighted avg     0.3865    0.6217    0.4767      3270\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dev_preds = clf.predict(dev_questions)\n",
    "print(classification_report(dev_answers, dev_preds, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CEYRTAsG-COB"
   },
   "source": [
    "## 2.2 Fasttext\n",
    "В качестве более сложной модели, использующей признаки вопроса и контекста используется модель логистической регрессии, построенной на векторах FastText вопроса и контекста. Для токенов каждой последовательности были получены векторные представления, которые затем были осреднены. Полученные два вектора затем конкатенировались и подавались на вход классификатору."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ONVp7rYisU2"
   },
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ssh1rt_viqMx"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# feature extractor\n",
    "from gensim.models import FastText\n",
    "\n",
    "# предобработка текстов\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "# progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "model = FastText.load_fasttext_format(\"wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ytIBc_0LJlL5"
   },
   "source": [
    "#### Токенизация, лемматизация и векторизация токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TSBq6IQ8AiRx"
   },
   "outputs": [],
   "source": [
    "stopwords_ = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Функция, которая токенизирует и лемматизирует входной текст\n",
    "    return: list of lemmas\n",
    "    \"\"\"\n",
    "    return [word.lemma_ for word in nlp(text) if not word.is_punct]\n",
    "\n",
    "def get_text_vector(tokens: List[str], model) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Переводит последовательнось лемм в единый вектор\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vectors.append(model.wv[token])\n",
    "    # return np.mean(vectors, axis=0)\n",
    "    return np.hstack([np.mean(vectors, axis=0), np.max(vectors, axis=0), np.std(vectors, axis=0)])\n",
    "\n",
    "def preprocess_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Предобработка текста. Включает в себя:\n",
    "    1. приведение в нижний регистр\n",
    "    2. токенизацию\n",
    "    3. лемматизацию\n",
    "    4. удаление стоп слов\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    lemmas = tokenize(text)\n",
    "    return [lemma for lemma in lemmas if lemma not in stopwords_]\n",
    "\n",
    "def get_ds_vectors(dataset: List[dict], model) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Функция, которая составляет векторное представление для каждой пары \n",
    "    (вопрос, контекст)\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for doc in tqdm(dataset):\n",
    "        text = doc['question'] + \" \" + doc[\"passage\"]\n",
    "        lemmas = preprocess_text(text)\n",
    "        vectors.append(get_text_vector(lemmas, model))\n",
    "\n",
    "    vectors = np.vstack(vectors)\n",
    "\n",
    "    return vectors\n",
    "\n",
    "train_vectors = get_ds_vectors(train_dataset, model)\n",
    "dev_vectors = get_ds_vectors(dev_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "s-hES-I0OD6u",
    "outputId": "a4c8398e-82bf-40c5-90ca-930254e1fb51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False     0.5119    0.2951    0.3744      1237\n",
      "        True     0.6590    0.8288    0.7342      2033\n",
      "\n",
      "    accuracy                         0.6269      3270\n",
      "   macro avg     0.5854    0.5619    0.5543      3270\n",
      "weighted avg     0.6033    0.6269    0.5981      3270\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=500, solver=\"liblinear\")\n",
    "clf.fit(train_vectors, train_answers)\n",
    "dev_preds = clf.predict(dev_vectors)\n",
    "\n",
    "print(classification_report(dev_answers, dev_preds, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPGnPrMqXktg"
   },
   "source": [
    "# Часть 3. Эмбеддинги предложений\n",
    "В данном разделе предлагается использовать модель логистической регрессии, обученной на конкатенированных признаках, полученных с помощью BERT. Модель BERT для английского языка была взята из библиотеки Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJJTiuikcCZG"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import pipeline\n",
    "import torch \n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "feature_extraction = pipeline(\"feature-extraction\", model=bert_model, tokenizer=bert_tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oO2Qfa1_MhXB"
   },
   "source": [
    "В качестве векторного представления последовательности токенов было взято среднее векторных представлений токенов, а не вектор соответствующий служебному токену [CLS]. Данное решение было принято в ходе эксперимента, так как модель с классификатор, построенный на выходе токена [CLS] показала accuracy на 0.03 меньше. Векторы вопроса и контекста затем конкатенируются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EODxiE71jceg"
   },
   "outputs": [],
   "source": [
    "def extract_features(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Составляет векторное представление для входной последовательности. \n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for text in tqdm(texts):\n",
    "        features.append(np.asarray(feature_extraction(text)).mean(axis=1)[0, :])\n",
    "    return np.vstack(features)\n",
    "\n",
    "# Получим вектора для вопроса и контекста поотдельности\n",
    "train_question_features = extract_features([d['question'] for d in train_dataset])\n",
    "train_passage_features = extract_features([d['passage'] for d in train_dataset])\n",
    "\n",
    "dev_question_features = extract_features([d['question'] for d in dev_dataset])\n",
    "dev_passage_features = extract_features([d['passage'] for d in dev_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpYXff3TNT6i"
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "NIeAW_NOZjmO",
    "outputId": "ed154e30-63e8-44ed-e1c9-1ac6fe154150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False     0.5702    0.4697    0.5151      1237\n",
      "        True     0.7086    0.7846    0.7446      2033\n",
      "\n",
      "    accuracy                         0.6654      3270\n",
      "   macro avg     0.6394    0.6271    0.6299      3270\n",
      "weighted avg     0.6562    0.6654    0.6578      3270\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define classifier\n",
    "clf = LogisticRegression(max_iter=10000, solver=\"liblinear\")\n",
    "\n",
    "# prepare train vetors\n",
    "train_vectors = np.hstack((train_question_features, train_passage_features))\n",
    "\n",
    "# fit logistic regression model\n",
    "clf.fit(train_vectors, train_answers)\n",
    "\n",
    "# validate on dev dataset\n",
    "dev_vectors = np.hstack((dev_question_features, dev_passage_features))\n",
    "dev_preds = clf.predict(dev_vectors)\n",
    "\n",
    "# print out results\n",
    "print(classification_report(dev_answers, dev_preds, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJJLR-zDGURm"
   },
   "source": [
    "# Часть 4. DrQA подобная архитектура"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TM5EZUBtQsee"
   },
   "source": [
    "Код для DrQA подобной архитектуры расположен в отдельном [репозитории](https://github.com/Muhamob/DrQA-1). Данный репозиторий является копией репозитория с уже реализованной архитектурой DrQA для задачи поиска начала и конца ответа. Части с чтением датасета, моделью и метриками была изменена под новый датасет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xNZ13ucUImC"
   },
   "source": [
    "### Архитектура\n",
    "\n",
    "Для возможности использования архитектуры DrQA для задачи проверки истинности высказывания, можно в качестве одного из вариантов архитектуры использовать следующий:\n",
    "1. убрать часть с предсказанием начала и конца ответа\n",
    "2. Аггрегировать выходы BiLSTM кодировщика контекста (например, взять среднее или другие аггрегирующие функции)\n",
    "3. Объединить признаки вопроса и контекста (конкатенировать)\n",
    "4. Использовать классификатор на основе получившихся признаков (в данной работе используется линейный слой с 1 выходным признаком).\n",
    "\n",
    "Также можно предлодить другие варианты архитектур. Например использовать векторы *similarity* для реккурентной нейронной сети. Затем, использовать выход последнего звена для предсказания ответа. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4s7WEAaRJ5R"
   },
   "source": [
    "### Результаты\n",
    "\n",
    "[эксперимент](https://github.com/Muhamob/abbyy_nlp_advanced/blob/master/question_answering/DrQA_BoolQ.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ltsjXb1RQ4P"
   },
   "source": [
    "Результаты работы для двух конфигураций следующие:\n",
    "```yaml\n",
    "- model:\n",
    "    doc layers: 2\n",
    "    question layers: 2\n",
    "    hidden size: 64\n",
    "    grad clipping: 3\n",
    "  best_accuracy: 0.6889\n",
    "- model:\n",
    "    doc layers: 3\n",
    "    question layers: 3\n",
    "    hidden size: 64\n",
    "    grad clipping: 3\n",
    "  best_accuracy: 0.6923 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PPSgyAuaSPUZ"
   },
   "source": [
    "### Недостатки и возможные пути улучшения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4KeoQKGSSUaQ"
   },
   "source": [
    "Датасет для данной задачи является недостаточным, так что при обучении данных моделей, возможно, возникало переобучение. \n",
    "\n",
    "Недочёты моей реализации модели:\n",
    "1. Отсутствие кросс-валидации. Валидация происходит только на одной отложенной выборке\n",
    "2. Отсутствие подбора гиперпараметров\n",
    "\n",
    "Возможности для улучшения качества модели:\n",
    "1. Использование предобученных моделей (например, на датасете SQuAD и др.)\n",
    "2. Подбор гиперпараметров\n",
    "3. Аугментация данных\n",
    "\n",
    "Гипотезы, которые хотелось бы проверить:\n",
    "1. Использование BERT для извлечения признаков токенов\n",
    "2. Использование другой архитектуры (предложена в разделе Архитектура)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuua-4CnQrLa"
   },
   "source": [
    "# Часть 5. BiDAF подобная архитекутра"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично предыдущему пункту, была адаптирована уже существующая архитектура из репозитория [galsang/BiDAF-pytorch](https://github.com/galsang/BiDAF-pytorch), [адаптация](https://github.com/Muhamob/BiDAF-pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура\n",
    "\n",
    "Для возможности использования архитектуры BiDAF для задачи проверки истинности высказывания, можно в качестве одного из вариантов архитектуры использовать следующий:\n",
    "1. убрать часть с предсказанием начала и конца ответа (Output Layer)\n",
    "2. Аггрегировать выходы BiLSTM (Modeling Layer) (например, взять среднее или другие аггрегирующие функции)\n",
    "4. Использовать классификатор на основе получившихся признаков (в данной работе используется линейный слой с 1 выходным признаком).\n",
    "\n",
    "Также можно предложить другие варианты архитектур:\n",
    "* Использовать не среднее выхода Modeling Layer, а конкатенацию первого и последнего вектора\n",
    "* Построить ещё одну RNN поверх выходов Modeling Layer и использовать последний вектор последовательности\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[эксперимент](https://github.com/Muhamob/abbyy_nlp_advanced/blob/master/question_answering/BiDAF_BoolQ_refactor.ipynb)\n",
    "\n",
    "модель\n",
    "```yaml\n",
    "tokenizer: nltk.word_tokenizer\n",
    "lemmatizer: None\n",
    "char_dim: 8\n",
    "char_channel_width: 5\n",
    "char_channel_size: 100\n",
    "context_threshold: 0\n",
    "dev_batch_size: 100\n",
    "dropout: 0.1\n",
    "epoch: 25\n",
    "exp_decay_rate: 0.999\n",
    "gpu: 0\n",
    "hidden_size: 100\n",
    "grad_clipping: 3\n",
    "weight_decay: 0\n",
    "print_freq: 250\n",
    "train_batch_size: 32\n",
    "word_dim: 100\n",
    "```\n",
    "результат: \n",
    "```yaml\n",
    "accuracy: 0.662\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемы\n",
    "\n",
    "1. Проблема с размером словаря символов, так как количество уникальных символов 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на то, что задача является достаточно простой для человека, у алгоритмов NLP она вызывает трудности. По моему небольшому опыту, классические алгоритмы (например TfIdf/FastText + LogReg) показывают достаточно хороший baseline в задачах классификации текстов. Однако в данной задаче baseline алгоритм работал немногим лучше случайного угадывания. \n",
    "\n",
    "Логистическая регрессия построенная на признаках BERT показала себя гораздо лучше, чем на признаках FastText. Стоит отметить, что по качеству она сопоставима с BiDAF подобной моделью. Однако, возможно, последняя модель могла бы показать лучшее качество, если сделать более качественную предобработку текста (ограничить словарь символов). Модель с DrQA подобной архитектурой показала себя лучше всех (accuracy = 0.6923 на отложенной выборке).\n",
    "\n",
    "Интересно было бы попробовать заменить входные признаки BiDAF и DrQA подобных архитектур на BERT признаки. Возможно, также как и с моделью логистической регрессии, это бы значительно улучшило качество. \n",
    "\n",
    "Интересно сравнить с SOTA моделями для данного датасета. Согласно сайту paperswithcode.com, наилучшая модель (T5-11B) показывает результат 91% accuracy, что значительно превосходит результаты моих запусков (~70 accuracy).\n",
    "\n",
    "В заключение хотелось бы добавить возможные пути улучшения качества работы последних двух моделей, которые, к сожалению, не было возможности проверить:\n",
    "1. Использование BERT признаков токенов в качестве исходных признаков\n",
    "2. Использование предобученных моделей. В данной работе модели обучались только на этом датасете\n",
    "3. Исследование различных параметров классифицирующего слоя\n",
    "4. Подбор гиперпараметров"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
