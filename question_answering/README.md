# Вопросно-ответная система для задачи подтверждения истинности высказывания

В данном задании необходимо исследовать датасет BoolQ, состоящий из триплетов: вопрос, контекст, ответ. При этом допускается ряд допущений:
	1. Вопросы подразумевают собой ответы Да/Нет.
	2. Ответ на вопрос содержится в контексте.
	3. 96% вопросов начинается из набора специальных слов (9 слов).
Несмотря на кажущуюся простоту задачи, простые классификаторы, такие как FastText+Logistic Regression показывают себя не лучше, чем так называемые DummyClassifier.

## FastText + LogisticRegression

#### FastText
```
model: wiki.en.300d
```

#### Logistic Regression
```
solver: liblinear
max_iter: 500
```
#### Результаты
```
              precision    recall  f1-score   support

       False     0.5119    0.2951    0.3744      1237
        True     0.6590    0.8288    0.7342      2033

    accuracy                         0.6269      3270
   macro avg     0.5854    0.5619    0.5543      3270
weighted avg     0.6033    0.6269    0.5981      3270
```
#### Выводы
Как видно из метрики, точность ответов незначительно отличается от точности при случайном угадывании

## BERT features + Logistic Regression
### Модели
#### BERT
Модель использовалась для извлечения признаков токенов вопроса и контекста. 
Модель была взята из библиотеки Transformers. 
Вектор текста - осреднение векторов токенов.
Признаки вопроса и контекста затем конкатенировались.
```
model: bert-base-uncased
sentence_vector: avg(model(tokens))
```
#### Logistic Regression
```
solver: liblinear
max_iter: 500
```
####
#### Результаты
```
              precision    recall  f1-score   support

       False     0.5702    0.4697    0.5151      1237
        True     0.7086    0.7846    0.7446      2033

    accuracy                         0.6654      3270
   macro avg     0.6394    0.6271    0.6299      3270
weighted avg     0.6562    0.6654    0.6578      3270
```
#### Выводы
В отличии от предудущей модели, данная модель отвечает на вопросы на 5% лучше.
## DrQA:
Данная архитектура изначально создавалась для поиска спана ответа в некотором абзаце. Однако, её можно адаптировать под данную задачу путём отброса слоёв, отвечающих за поиск начала и конца ответа, и заменив её на классификатор.

#### checklist
	[x] Предложить надстройку для данной задачи
	[x] Реализовать / переделать существующие решения
	[x] Настроить пути / датасеты / параметры / метрики
	[x] Получить результаты
	[x] Настроить гиперпараметры

#### Архитектура
х Здесь может быть ваша картинка с архитектурой ъ

#### Результаты
##### Accuracy
```
- model:
    doc layers: 2
    question layers: 2
    hidden size: 64
    grad clipping: 3
  best_accuracy: 0.6889
- model:
    doc layers: 3
    question layers: 3
    hidden size: 64
    grad clipping: 3
  best_accuracy: 0.6923 
```
#### Выводы
Модель показала себя немногим лучше, чем предыдущая

## BiDAF

Архитектура, аналогично предыдущей создавалсь для поиска спанов ответа в контексте. Однако заменив часть с поиском спанов, на слои для задачи бинарной классификации, можно применить архитектуру к данному датасету

#### checklist
	[x] Предложить надстройку для данной задачи
	[x] Реализовать / переделать существующие решения
	[x] Настроить пути / датасеты / параметры / метрики
	[x] Получить результаты
	[x] Настроить гиперпараметры

#### Архитектура
х Здесь может быть ваша картинка с архитектурой ъ

#### Результаты
##### Accuracy
Эксперименты:
-   модель:
    ```yaml
    tokenizer: nltk.word_tokenizer
    lemmatizer: None
    char_dim: 8
    char_channel_width: 5
    char_channel_size: 100
    context_threshold: 0
    dev_batch_size: 100
    dropout: 0.1
    epoch: 25
    exp_decay_rate: 0.999
    gpu: 0
    hidden_size: 100
    grad_clipping: 3
    weight_decay: 0
    print_freq: 250
    train_batch_size: 32
    word_dim: 100
    ```
    результат: 
    ```yaml
    accuracy: 0.662
    ```
-   модель
    ```yaml
    
    ```